{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- dataset: Awesome075/multi_news_parquet\n",
    "- model: Qwen/Qwen2.5-7B-Instruct\n",
    "- task: Multi-document summarization (English)\n",
    "\n",
    "(used 'fine-tuning' conda environment in RTX5090 server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig, PeftModel\n",
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import wandb\nwandb.init(\n    project=\"qwen25-multinews-finetuning\",\n    name=\"lora-7b-instruct\",\n    config={\n        \"model\": \"Qwen/Qwen2.5-7B-Instruct\",\n        \"dataset\": \"Awesome075/multi_news_parquet\",\n        \"task\": \"multi-document-summarization\",\n    }\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System message\n",
    "system_message = \"\"\"You are a professional summarization model.\n",
    "You create concise, accurate summaries that capture key information from multiple news articles.\"\"\"\n",
    "\n",
    "# Best prompt from evaluate_prompts.py (prompt_2)\n",
    "prompt = \"\"\"The following news articles cover the same event. Read all articles and provide a comprehensive summary.\n",
    "\n",
    "{document}\n",
    "\n",
    "Provide a summary that captures the main points:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample, prompt_template, max_doc_length=6000):\n",
    "    \"\"\"Multi-News data to training format\n",
    "    \n",
    "    Args:\n",
    "        sample: dataset sample with 'document' and 'summary' fields\n",
    "        prompt_template: prompt with {document} placeholder\n",
    "        max_doc_length: maximum character length for documents (truncation)\n",
    "    \"\"\"\n",
    "    document = sample[\"document\"]\n",
    "    \n",
    "    # Truncate if too long (documents are ~2000+ words)\n",
    "    if len(document) > max_doc_length:\n",
    "        document = document[:max_doc_length]\n",
    "        last_period = document.rfind('.')\n",
    "        if last_period > max_doc_length * 0.8:\n",
    "            document = document[:last_period + 1]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_message,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt_template.format(document=document),\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": sample[\"summary\"].strip(),\n",
    "            },\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Multi-News dataset (Parquet version for compatibility)\n",
    "dataset = load_dataset(\"Awesome075/multi_news_parquet\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# Sample check\n",
    "print(\"\\nSample data:\")\n",
    "sample = train_dataset[0]\n",
    "print(f\"Document length: {len(sample['document'].split())} words\")\n",
    "print(f\"Summary length: {len(sample['summary'].split())} words\")\n",
    "print(f\"Number of source docs: {sample['document'].count('|||||') + 1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model and tokenizer loading\nmodel_id = \"Qwen/Qwen2.5-7B-Instruct\"\noutput_dir = \"qwen25-multinews-lora\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n# Ensure padding token is set\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    tokenizer.pad_token_id = tokenizer.eos_token_id\n\n# Verify chat template\ntext = tokenizer.apply_chat_template(\n    [{\"role\": \"user\", \"content\": \"test\"}],\n    tokenize=False,\n    add_generation_prompt=True\n)\nprint(\"Chat template sample:\")\nprint(text)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def compute_rouge(predictions, references):\n    \"\"\"Compute ROUGE scores for summarization evaluation\n    \n    Args:\n        predictions: list of generated summaries\n        references: list of reference summaries\n    \n    Returns:\n        dict with ROUGE-1, ROUGE-2, ROUGE-L F1 scores\n    \"\"\"\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    \n    rouge1_scores = []\n    rouge2_scores = []\n    rougeL_scores = []\n    \n    for pred, ref in zip(predictions, references):\n        scores = scorer.score(ref, pred)\n        rouge1_scores.append(scores['rouge1'].fmeasure)\n        rouge2_scores.append(scores['rouge2'].fmeasure)\n        rougeL_scores.append(scores['rougeL'].fmeasure)\n    \n    return {\n        'rouge1': sum(rouge1_scores) / len(rouge1_scores),\n        'rouge2': sum(rouge2_scores) / len(rouge2_scores),\n        'rougeL': sum(rougeL_scores) / len(rougeL_scores),\n    }\n\n\ndef evaluate_model(model, tokenizer, test_samples, num_samples=100, max_new_tokens=300, batch_size=4):\n    \"\"\"Evaluate model on test samples using ROUGE scores with batch processing\"\"\"\n    model.eval()\n    total = min(num_samples, len(test_samples))\n    \n    predictions = []\n    references = []\n    \n    # Process in batches for efficiency\n    for batch_start in range(0, total, batch_size):\n        batch_end = min(batch_start + batch_size, total)\n        batch_samples = test_samples[batch_start:batch_end]\n        \n        # Prepare batch inputs\n        batch_texts = []\n        batch_refs = []\n        for sample in batch_samples:\n            messages = sample[\"messages\"][:2]\n            text = tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=True\n            )\n            batch_texts.append(text)\n            batch_refs.append(sample[\"messages\"][2][\"content\"].strip())\n        \n        # Tokenize batch with left padding for generation\n        tokenizer.padding_side = \"left\"\n        inputs = tokenizer(\n            batch_texts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=4096,\n        ).to(model.device)\n        \n        # Generate\n        with torch.no_grad():\n            generated_ids = model.generate(\n                **inputs,\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n                pad_token_id=tokenizer.pad_token_id,\n            )\n        \n        # Decode outputs (skip input tokens for each sample)\n        for i, (gen_ids, input_len) in enumerate(zip(generated_ids, inputs.input_ids)):\n            # Find actual input length (excluding padding)\n            actual_input_len = (input_len != tokenizer.pad_token_id).sum().item()\n            output = tokenizer.decode(\n                gen_ids[inputs.input_ids.shape[1]:],\n                skip_special_tokens=True\n            ).strip()\n            predictions.append(output)\n        \n        references.extend(batch_refs)\n        \n        if (batch_end) % 20 == 0 or batch_end == total:\n            print(f\"Evaluated {batch_end}/{total} samples\")\n    \n    # Reset padding side\n    tokenizer.padding_side = \"right\"\n    \n    # Compute ROUGE scores\n    scores = compute_rouge(predictions, references)\n    return scores, predictions, references"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare formatted datasets\n",
    "# Use subset for faster training (optional - can use full dataset)\n",
    "train_subset_size = 10000  # Adjust based on compute resources\n",
    "val_subset_size = 1000\n",
    "\n",
    "train_formatted = [\n",
    "    format_data(row, prompt)\n",
    "    for row in train_dataset.select(range(min(train_subset_size, len(train_dataset))))\n",
    "]\n",
    "val_formatted = [\n",
    "    format_data(row, prompt)\n",
    "    for row in val_dataset.select(range(min(val_subset_size, len(val_dataset))))\n",
    "]\n",
    "test_formatted = [\n",
    "    format_data(row, prompt)\n",
    "    for row in test_dataset.select(range(1000))  # 1000 samples for evaluation\n",
    "]\n",
    "\n",
    "print(f\"Train formatted: {len(train_formatted)} samples\")\n",
    "print(f\"Validation formatted: {len(val_formatted)} samples\")\n",
    "print(f\"Test formatted: {len(test_formatted)} samples\")\n",
    "\n",
    "# Convert to Dataset objects\n",
    "train_formatted = Dataset.from_list(train_formatted)\n",
    "val_formatted = Dataset.from_list(val_formatted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Maximum sequence length (important for long documents)\nmax_seq_length = 4096  # Qwen2.5 supports up to 32K, but 4096 is reasonable for memory\n\ndef collate_fn(batch):\n    \"\"\"\n    Collate function with proper label masking for Qwen2.5 chat template.\n    Only compute loss on assistant response tokens.\n    Uses string-based search for efficiency.\n    \"\"\"\n    new_batch = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        \"labels\": []\n    }\n    \n    # Qwen2.5 chat template markers\n    assistant_start_str = \"<|im_start|>assistant\\n\"\n    assistant_end_str = \"<|im_end|>\"\n    \n    for example in batch:\n        # Apply chat template\n        text = tokenizer.apply_chat_template(\n            example[\"messages\"],\n            tokenize=False,\n            add_generation_prompt=False\n        )\n        \n        # Find assistant response boundaries in text (more efficient)\n        assistant_start_pos = text.find(assistant_start_str)\n        if assistant_start_pos == -1:\n            # Fallback: no assistant response found, mask everything\n            tokenized = tokenizer(\n                text,\n                truncation=True,\n                max_length=max_seq_length,\n                padding=False,\n                return_tensors=None,\n            )\n            input_ids = tokenized[\"input_ids\"]\n            attention_mask = tokenized[\"attention_mask\"]\n            labels = [-100] * len(input_ids)\n        else:\n            # Split text into prefix (to mask) and response (to learn)\n            prefix_text = text[:assistant_start_pos + len(assistant_start_str)]\n            response_text = text[assistant_start_pos + len(assistant_start_str):]\n            \n            # Tokenize prefix\n            prefix_tokens = tokenizer(\n                prefix_text,\n                add_special_tokens=True,\n                truncation=True,\n                max_length=max_seq_length,\n                padding=False,\n                return_tensors=None,\n            )\n            \n            # Tokenize full text\n            tokenized = tokenizer(\n                text,\n                truncation=True,\n                max_length=max_seq_length,\n                padding=False,\n                return_tensors=None,\n            )\n            \n            input_ids = tokenized[\"input_ids\"]\n            attention_mask = tokenized[\"attention_mask\"]\n            \n            # Mask prefix tokens, keep response tokens\n            prefix_len = len(prefix_tokens[\"input_ids\"])\n            labels = [-100] * prefix_len + input_ids[prefix_len:]\n            \n            # Ensure labels length matches input_ids\n            if len(labels) < len(input_ids):\n                labels = labels + input_ids[len(labels):]\n            elif len(labels) > len(input_ids):\n                labels = labels[:len(input_ids)]\n        \n        new_batch[\"input_ids\"].append(input_ids)\n        new_batch[\"attention_mask\"].append(attention_mask)\n        new_batch[\"labels\"].append(labels)\n    \n    # Apply padding\n    max_length = max(len(ids) for ids in new_batch[\"input_ids\"])\n    \n    for i in range(len(new_batch[\"input_ids\"])):\n        padding_length = max_length - len(new_batch[\"input_ids\"][i])\n        new_batch[\"input_ids\"][i].extend([tokenizer.pad_token_id] * padding_length)\n        new_batch[\"attention_mask\"][i].extend([0] * padding_length)\n        new_batch[\"labels\"][i].extend([-100] * padding_length)\n    \n    # Convert to tensors\n    for k, v in new_batch.items():\n        new_batch[k] = torch.tensor(v)\n    \n    return new_batch"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LoRA configuration (7B is large, so LoRA only)\npeft_config = LoraConfig(\n    lora_alpha=64,\n    lora_dropout=0.05,\n    r=32,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n    task_type=\"CAUSAL_LM\",\n)\n\n# Training configuration\nlora_args = SFTConfig(\n    output_dir=output_dir,\n    num_train_epochs=2,\n    per_device_train_batch_size=2,      # Small batch for 7B model\n    per_device_eval_batch_size=2,\n    gradient_accumulation_steps=8,       # Effective batch size = 16\n    gradient_checkpointing=True,         # Save memory\n    optim=\"adamw_torch_fused\",\n    logging_steps=50,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"epoch\",\n    bf16=True,\n    learning_rate=1e-4,                  # Higher LR for LoRA\n    max_grad_norm=0.3,\n    warmup_ratio=0.03,\n    lr_scheduler_type=\"cosine\",\n    max_seq_length=max_seq_length,\n    remove_unused_columns=False,\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n    report_to=\"wandb\",\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer and train\n",
    "lora_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=lora_args,\n",
    "    train_dataset=train_formatted,\n",
    "    eval_dataset=val_formatted,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "lora_trainer.train()\n",
    "\n",
    "# Save model\n",
    "lora_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 200  # Number of test samples for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base model evaluation\n",
    "print(\"Loading Base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(\"Evaluating Base model...\")\n",
    "base_scores, base_preds, base_refs = evaluate_model(\n",
    "    base_model, tokenizer, test_formatted, num_samples=NUM_SAMPLES\n",
    ")\n",
    "print(f\"\\nBase model ROUGE scores:\")\n",
    "print(f\"  ROUGE-1: {base_scores['rouge1']:.4f}\")\n",
    "print(f\"  ROUGE-2: {base_scores['rouge2']:.4f}\")\n",
    "print(f\"  ROUGE-L: {base_scores['rougeL']:.4f}\")\n",
    "\n",
    "del base_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# LoRA model evaluation\nprint(\"Loading LoRA model...\")\nbase_model = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    torch_dtype=torch.bfloat16,\n)\nlora_model = PeftModel.from_pretrained(base_model, output_dir)\n\nprint(\"Evaluating LoRA model...\")\nlora_scores, lora_preds, lora_refs = evaluate_model(\n    lora_model, tokenizer, test_formatted, num_samples=NUM_SAMPLES\n)\nprint(f\"\\nLoRA model ROUGE scores:\")\nprint(f\"  ROUGE-1: {lora_scores['rouge1']:.4f}\")\nprint(f\"  ROUGE-2: {lora_scores['rouge2']:.4f}\")\nprint(f\"  ROUGE-L: {lora_scores['rougeL']:.4f}\")\n\n# Clean up memory\ndel lora_model, base_model\ntorch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results summary table\n",
    "print(\"\\n## Results Summary\\n\")\n",
    "print(\"| Model | ROUGE-1 | ROUGE-2 | ROUGE-L |\")\n",
    "print(\"|-------|---------|---------|---------|\")\n",
    "print(f\"| Base | {base_scores['rouge1']:.4f} | {base_scores['rouge2']:.4f} | {base_scores['rougeL']:.4f} |\")\n",
    "print(f\"| LoRA | {lora_scores['rouge1']:.4f} | {lora_scores['rouge2']:.4f} | {lora_scores['rougeL']:.4f} |\")\n",
    "\n",
    "# Improvement calculation\n",
    "print(f\"\\n### Improvement\")\n",
    "print(f\"ROUGE-1: {(lora_scores['rouge1'] - base_scores['rouge1']):+.4f}\")\n",
    "print(f\"ROUGE-2: {(lora_scores['rouge2'] - base_scores['rouge2']):+.4f}\")\n",
    "print(f\"ROUGE-L: {(lora_scores['rougeL'] - base_scores['rougeL']):+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qualitative examples\n",
    "print(\"\\n## Sample Predictions\\n\")\n",
    "for i in range(3):\n",
    "    print(f\"### Sample {i+1}\")\n",
    "    print(f\"**Reference:**\\n{lora_refs[i][:500]}...\")\n",
    "    print(f\"\\n**Base Model:**\\n{base_preds[i][:500]}...\")\n",
    "    print(f\"\\n**LoRA Model:**\\n{lora_preds[i][:500]}...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}