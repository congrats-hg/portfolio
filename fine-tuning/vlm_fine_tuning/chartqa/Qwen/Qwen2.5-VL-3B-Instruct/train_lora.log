nohup: ignoring input
wandb: [wandb.login()] Loaded credentials for https://api.wandb.ai from /home/ryu5090/.netrc.
wandb: Currently logged in as: hagyeong929 (hagyeong929-kyonggi-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: setting up run mx3x1rvz
wandb: Tracking run with wandb version 0.24.0
wandb: Run data is saved locally in /home/ryu5090/dev/portfolio/fine-tuning/vlm_fine_tuning/chartqa/Qwen/Qwen2.5-VL-3B-Instruct/wandb/run-20260125_202412-mx3x1rvz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run lyric-water-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning
wandb: üöÄ View run at https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning/runs/mx3x1rvz
loading configuration file config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/config.json
`torch_dtype` is deprecated! Use `dtype` instead!
Model config Qwen2_5_VLConfig {
  "architectures": [
    "Qwen2_5_VLForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 151643,
  "dtype": "bfloat16",
  "eos_token_id": 151645,
  "hidden_act": "silu",
  "hidden_size": 2048,
  "image_token_id": 151655,
  "initializer_range": 0.02,
  "intermediate_size": 11008,
  "max_position_embeddings": 128000,
  "max_window_layers": 70,
  "model_type": "qwen2_5_vl",
  "num_attention_heads": 16,
  "num_hidden_layers": 36,
  "num_key_value_heads": 2,
  "rms_norm_eps": 1e-06,
  "rope_scaling": {
    "mrope_section": [
      16,
      24,
      24
    ],
    "rope_type": "default",
    "type": "default"
  },
  "rope_theta": 1000000.0,
  "sliding_window": 32768,
  "text_config": {
    "architectures": [
      "Qwen2_5_VLForConditionalGeneration"
    ],
    "attention_dropout": 0.0,
    "bos_token_id": 151643,
    "dtype": "bfloat16",
    "eos_token_id": 151645,
    "hidden_act": "silu",
    "hidden_size": 2048,
    "initializer_range": 0.02,
    "intermediate_size": 11008,
    "layer_types": [
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention",
      "full_attention"
    ],
    "max_position_embeddings": 128000,
    "max_window_layers": 70,
    "model_type": "qwen2_5_vl_text",
    "num_attention_heads": 16,
    "num_hidden_layers": 36,
    "num_key_value_heads": 2,
    "rms_norm_eps": 1e-06,
    "rope_scaling": {
      "mrope_section": [
        16,
        24,
        24
      ],
      "rope_type": "default",
      "type": "default"
    },
    "rope_theta": 1000000.0,
    "sliding_window": null,
    "tie_word_embeddings": true,
    "use_cache": true,
    "use_sliding_window": false,
    "vision_token_id": 151654,
    "vocab_size": 151936
  },
  "transformers_version": "4.57.6",
  "use_cache": true,
  "use_sliding_window": false,
  "video_token_id": 151656,
  "vision_config": {
    "depth": 32,
    "fullatt_block_indexes": [
      7,
      15,
      23,
      31
    ],
    "hidden_act": "silu",
    "hidden_size": 1280,
    "in_channels": 3,
    "in_chans": 3,
    "initializer_range": 0.02,
    "intermediate_size": 3420,
    "model_type": "qwen2_5_vl",
    "num_heads": 16,
    "out_hidden_size": 2048,
    "patch_size": 14,
    "spatial_merge_size": 2,
    "spatial_patch_size": 14,
    "temporal_patch_size": 2,
    "tokens_per_second": 2,
    "window_size": 112
  },
  "vision_end_token_id": 151653,
  "vision_start_token_id": 151652,
  "vision_token_id": 151654,
  "vocab_size": 151936
}

loading weights file model.safetensors from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/model.safetensors.index.json
Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "eos_token_id": 151645
}

Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.
Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.
Loading dataset...
Train: 28299 samples
Val: 1920 samples
Test: 2500 samples
Formatting data...
Loading model and processor...
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 1/2 [00:01<00:01,  1.15s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.09s/it]Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:02<00:00,  1.09s/it]
loading configuration file generation_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/generation_config.json
Generate config GenerationConfig {
  "bos_token_id": 151643,
  "do_sample": true,
  "eos_token_id": [
    151645,
    151643
  ],
  "pad_token_id": 151643,
  "repetition_penalty": 1.05,
  "temperature": 1e-06
}

Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-3B-Instruct.
loading configuration file preprocessor_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json
loading configuration file preprocessor_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json
The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.
Image processor Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_pixels": 401408,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

loading file vocab.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/vocab.json
loading file merges.txt from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/merges.txt
loading file tokenizer.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
loading configuration file video_preprocessor_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json
Video processor Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 401408,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}

loading configuration file processor_config.json from cache at None
Processor Qwen2_5_VLProcessor:
- image_processor: Qwen2VLImageProcessorFast {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "disable_grouping": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_pad": null,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "Qwen2VLImageProcessorFast",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_pixels": 401408,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_tensors": null,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2
}

- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={
	151643: AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151644: AddedToken("<|im_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151645: AddedToken("<|im_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151646: AddedToken("<|object_ref_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151647: AddedToken("<|object_ref_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151648: AddedToken("<|box_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151649: AddedToken("<|box_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151650: AddedToken("<|quad_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151651: AddedToken("<|quad_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151652: AddedToken("<|vision_start|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151653: AddedToken("<|vision_end|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151654: AddedToken("<|vision_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151655: AddedToken("<|image_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151656: AddedToken("<|video_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),
	151657: AddedToken("<tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151658: AddedToken("</tool_call>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151659: AddedToken("<|fim_prefix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151660: AddedToken("<|fim_middle|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151661: AddedToken("<|fim_suffix|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151662: AddedToken("<|fim_pad|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151663: AddedToken("<|repo_name|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
	151664: AddedToken("<|file_sep|>", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),
}
)
- video_processor: Qwen2VLVideoProcessor {
  "crop_size": null,
  "data_format": "channels_first",
  "default_to_square": true,
  "device": null,
  "do_center_crop": null,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "do_sample_frames": false,
  "fps": null,
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "input_data_format": null,
  "max_frames": 768,
  "max_pixels": 401408,
  "merge_size": 2,
  "min_frames": 4,
  "min_pixels": 401408,
  "num_frames": null,
  "pad_size": null,
  "patch_size": 14,
  "processor_class": "Qwen2_5_VLProcessor",
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "return_metadata": false,
  "size": {
    "longest_edge": 12845056,
    "shortest_edge": 3136
  },
  "temporal_patch_size": 2,
  "video_metadata": null,
  "video_processor_type": "Qwen2VLVideoProcessor"
}


{
  "processor_class": "Qwen2_5_VLProcessor"
}

PyTorch: setting up devices
You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.
Using auto half precision backend
The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.
***** Running training *****
  Num examples = 28,299
  Num Epochs = 2
  Instantaneous batch size per device = 8
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 4
  Total optimization steps = 1,770
  Number of trainable parameters = 74,305,536
Automatic Weights & Biases logging enabled, to disable set os.environ["WANDB_DISABLED"] = "true"
Starting LoRA fine-tuning...
  0%|          | 0/1770 [00:00<?, ?it/s]  0%|          | 1/1770 [00:15<7:50:58, 15.97s/it]  0%|          | 2/1770 [00:30<7:22:14, 15.01s/it]  0%|          | 3/1770 [00:44<7:14:48, 14.76s/it]  0%|          | 4/1770 [00:59<7:08:46, 14.57s/it]  0%|          | 5/1770 [01:14<7:13:40, 14.74s/it]  0%|          | 6/1770 [01:28<7:09:30, 14.61s/it]  0%|          | 7/1770 [01:42<7:08:05, 14.57s/it]  0%|          | 8/1770 [01:57<7:05:02, 14.47s/it]  1%|          | 9/1770 [02:11<7:05:30, 14.50s/it]  1%|          | 10/1770 [02:26<7:04:22, 14.47s/it]  1%|          | 11/1770 [02:41<7:09:34, 14.65s/it]  1%|          | 12/1770 [02:55<7:07:06, 14.58s/it]  1%|          | 13/1770 [03:09<7:04:39, 14.50s/it]  1%|          | 14/1770 [03:24<7:04:19, 14.50s/it]  1%|          | 15/1770 [03:38<7:03:07, 14.47s/it]  1%|          | 16/1770 [03:53<7:02:55, 14.47s/it]  1%|          | 17/1770 [04:08<7:08:36, 14.67s/it]  1%|          | 18/1770 [04:22<7:06:24, 14.60s/it]  1%|          | 19/1770 [04:37<7:04:34, 14.55s/it]  1%|          | 20/1770 [04:52<7:08:07, 14.68s/it]  1%|          | 21/1770 [05:06<7:06:01, 14.61s/it]  1%|          | 22/1770 [05:21<7:05:08, 14.59s/it]  1%|‚ñè         | 23/1770 [05:35<7:03:59, 14.56s/it]  1%|‚ñè         | 24/1770 [05:50<7:04:10, 14.58s/it]  1%|‚ñè         | 25/1770 [06:04<7:03:28, 14.56s/it]  1%|‚ñè         | 26/1770 [06:19<7:01:28, 14.50s/it]  2%|‚ñè         | 27/1770 [06:34<7:06:48, 14.69s/it]  2%|‚ñè         | 28/1770 [06:49<7:05:55, 14.67s/it]  2%|‚ñè         | 29/1770 [07:03<7:03:31, 14.60s/it]  2%|‚ñè         | 30/1770 [07:18<7:02:53, 14.58s/it]  2%|‚ñè         | 31/1770 [07:32<7:03:43, 14.62s/it]  2%|‚ñè         | 32/1770 [07:47<7:06:25, 14.72s/it]  2%|‚ñè         | 33/1770 [08:02<7:05:15, 14.69s/it]  2%|‚ñè         | 34/1770 [08:16<7:04:09, 14.66s/it]  2%|‚ñè         | 35/1770 [08:31<7:02:31, 14.61s/it]  2%|‚ñè         | 36/1770 [08:45<7:01:09, 14.57s/it]  2%|‚ñè         | 37/1770 [09:00<7:00:24, 14.56s/it]  2%|‚ñè         | 38/1770 [09:15<7:03:49, 14.68s/it]  2%|‚ñè         | 39/1770 [09:29<7:02:13, 14.63s/it]  2%|‚ñè         | 40/1770 [09:44<6:59:28, 14.55s/it]  2%|‚ñè         | 41/1770 [09:58<6:59:02, 14.54s/it]  2%|‚ñè         | 42/1770 [10:13<6:57:38, 14.50s/it]  2%|‚ñè         | 43/1770 [10:27<6:57:33, 14.51s/it]  2%|‚ñè         | 44/1770 [10:42<6:57:05, 14.50s/it]  3%|‚ñé         | 45/1770 [10:57<7:01:36, 14.66s/it]  3%|‚ñé         | 46/1770 [11:11<6:59:11, 14.59s/it]  3%|‚ñé         | 47/1770 [11:26<6:57:48, 14.55s/it]  3%|‚ñé         | 48/1770 [11:40<6:57:26, 14.54s/it]  3%|‚ñé         | 49/1770 [11:55<6:55:41, 14.49s/it]  3%|‚ñé         | 50/1770 [12:10<7:01:14, 14.69s/it]                                                     3%|‚ñé         | 50/1770 [12:10<7:01:14, 14.69s/it]  3%|‚ñé         | 51/1770 [12:24<7:00:07, 14.66s/it]  3%|‚ñé         | 52/1770 [12:39<6:59:02, 14.64s/it]  3%|‚ñé         | 53/1770 [12:53<6:56:37, 14.56s/it]  3%|‚ñé         | 54/1770 [13:08<6:55:13, 14.52s/it]  3%|‚ñé         | 55/1770 [13:22<6:55:31, 14.54s/it]  3%|‚ñé         | 56/1770 [13:37<6:59:41, 14.69s/it]  3%|‚ñé         | 57/1770 [13:52<6:57:52, 14.64s/it]  3%|‚ñé         | 58/1770 [14:06<6:56:32, 14.60s/it]  3%|‚ñé         | 59/1770 [14:21<6:55:11, 14.56s/it]  3%|‚ñé         | 60/1770 [14:35<6:53:44, 14.52s/it]  3%|‚ñé         | 61/1770 [14:50<6:54:06, 14.54s/it]  4%|‚ñé         | 62/1770 [15:04<6:52:50, 14.50s/it]  4%|‚ñé         | 63/1770 [15:19<6:52:48, 14.51s/it]  4%|‚ñé         | 64/1770 [15:33<6:52:28, 14.51s/it]  4%|‚ñé         | 65/1770 [15:48<6:57:18, 14.69s/it]  4%|‚ñé         | 66/1770 [16:03<6:56:31, 14.67s/it]  4%|‚ñç         | 67/1770 [16:17<6:54:30, 14.60s/it]  4%|‚ñç         | 68/1770 [16:32<6:52:02, 14.53s/it]  4%|‚ñç         | 69/1770 [16:46<6:52:04, 14.53s/it]  4%|‚ñç         | 70/1770 [17:01<6:55:56, 14.68s/it]  4%|‚ñç         | 71/1770 [17:16<6:53:22, 14.60s/it]  4%|‚ñç         | 72/1770 [17:30<6:53:02, 14.59s/it]  4%|‚ñç         | 73/1770 [17:45<6:51:57, 14.57s/it]  4%|‚ñç         | 74/1770 [17:59<6:50:05, 14.51s/it]  4%|‚ñç         | 75/1770 [18:14<6:54:07, 14.66s/it]  4%|‚ñç         | 76/1770 [18:29<6:52:50, 14.62s/it]  4%|‚ñç         | 77/1770 [18:43<6:49:45, 14.52s/it]  4%|‚ñç         | 78/1770 [18:58<6:49:20, 14.52s/it]  4%|‚ñç         | 79/1770 [19:12<6:49:02, 14.51s/it]  5%|‚ñç         | 80/1770 [19:27<6:48:34, 14.51s/it]  5%|‚ñç         | 81/1770 [19:41<6:48:21, 14.51s/it]  5%|‚ñç         | 82/1770 [19:56<6:53:17, 14.69s/it]  5%|‚ñç         | 83/1770 [20:11<6:51:38, 14.64s/it]  5%|‚ñç         | 84/1770 [20:25<6:50:38, 14.61s/it]  5%|‚ñç         | 85/1770 [20:40<6:49:14, 14.57s/it]  5%|‚ñç         | 86/1770 [20:55<6:53:42, 14.74s/it]  5%|‚ñç         | 87/1770 [21:09<6:51:28, 14.67s/it]  5%|‚ñç         | 88/1770 [21:24<6:50:27, 14.64s/it]  5%|‚ñå         | 89/1770 [21:38<6:48:40, 14.59s/it]  5%|‚ñå         | 90/1770 [21:53<6:46:19, 14.51s/it]  5%|‚ñå         | 91/1770 [22:08<6:50:24, 14.67s/it]  5%|‚ñå         | 92/1770 [22:22<6:49:13, 14.63s/it]  5%|‚ñå         | 93/1770 [22:37<6:47:24, 14.58s/it]  5%|‚ñå         | 94/1770 [22:51<6:46:21, 14.55s/it]  5%|‚ñå         | 95/1770 [23:06<6:46:08, 14.55s/it]  5%|‚ñå         | 96/1770 [23:21<6:49:57, 14.69s/it]  5%|‚ñå         | 97/1770 [23:35<6:47:31, 14.62s/it]  6%|‚ñå         | 98/1770 [23:50<6:46:25, 14.58s/it]  6%|‚ñå         | 99/1770 [24:04<6:45:49, 14.57s/it]  6%|‚ñå         | 100/1770 [24:19<6:44:52, 14.55s/it]                                                      6%|‚ñå         | 100/1770 [24:19<6:44:52, 14.55s/it]  6%|‚ñå         | 101/1770 [24:33<6:44:43, 14.55s/it]  6%|‚ñå         | 102/1770 [24:48<6:48:06, 14.68s/it]  6%|‚ñå         | 103/1770 [25:03<6:46:31, 14.63s/it]  6%|‚ñå         | 104/1770 [25:17<6:45:20, 14.60s/it]  6%|‚ñå         | 105/1770 [25:32<6:44:13, 14.57s/it]  6%|‚ñå         | 106/1770 [25:46<6:43:29, 14.55s/it]  6%|‚ñå         | 107/1770 [26:01<6:47:34, 14.71s/it]  6%|‚ñå         | 108/1770 [26:16<6:44:57, 14.62s/it]  6%|‚ñå         | 109/1770 [26:30<6:44:59, 14.63s/it]  6%|‚ñå         | 110/1770 [26:45<6:44:05, 14.61s/it]  6%|‚ñã         | 111/1770 [27:00<6:47:09, 14.73s/it]  6%|‚ñã         | 112/1770 [27:14<6:44:35, 14.64s/it]  6%|‚ñã         | 113/1770 [27:29<6:41:50, 14.55s/it]  6%|‚ñã         | 114/1770 [27:43<6:41:43, 14.56s/it]  6%|‚ñã         | 115/1770 [27:58<6:41:22, 14.55s/it]  7%|‚ñã         | 116/1770 [28:12<6:39:55, 14.51s/it]  7%|‚ñã         | 117/1770 [28:27<6:38:52, 14.48s/it]  7%|‚ñã         | 118/1770 [28:42<6:42:44, 14.63s/it]  7%|‚ñã         | 119/1770 [28:56<6:41:15, 14.58s/it]  7%|‚ñã         | 120/1770 [29:11<6:40:15, 14.55s/it]  7%|‚ñã         | 121/1770 [29:25<6:40:34, 14.58s/it]  7%|‚ñã         | 122/1770 [29:40<6:38:57, 14.52s/it]  7%|‚ñã         | 123/1770 [29:55<6:42:21, 14.66s/it]  7%|‚ñã         | 124/1770 [30:09<6:40:49, 14.61s/it]  7%|‚ñã         | 125/1770 [30:24<6:39:40, 14.58s/it]  7%|‚ñã         | 126/1770 [30:38<6:39:15, 14.57s/it]  7%|‚ñã         | 127/1770 [30:53<6:37:25, 14.51s/it]  7%|‚ñã         | 128/1770 [31:07<6:37:26, 14.52s/it]  7%|‚ñã         | 129/1770 [31:22<6:37:30, 14.53s/it]  7%|‚ñã         | 130/1770 [31:37<6:41:12, 14.68s/it]  7%|‚ñã         | 131/1770 [31:51<6:38:38, 14.59s/it]  7%|‚ñã         | 132/1770 [32:06<6:37:42, 14.57s/it]  8%|‚ñä         | 133/1770 [32:20<6:37:39, 14.57s/it]  8%|‚ñä         | 134/1770 [32:35<6:36:30, 14.54s/it]  8%|‚ñä         | 135/1770 [32:49<6:36:29, 14.55s/it]  8%|‚ñä         | 136/1770 [33:04<6:40:08, 14.69s/it]  8%|‚ñä         | 137/1770 [33:19<6:38:11, 14.63s/it]  8%|‚ñä         | 138/1770 [33:33<6:37:08, 14.60s/it]  8%|‚ñä         | 139/1770 [33:48<6:34:48, 14.52s/it]  8%|‚ñä         | 140/1770 [34:02<6:33:53, 14.50s/it]  8%|‚ñä         | 141/1770 [34:17<6:38:26, 14.68s/it]  8%|‚ñä         | 142/1770 [34:32<6:37:12, 14.64s/it]  8%|‚ñä         | 143/1770 [34:46<6:35:50, 14.60s/it]  8%|‚ñä         | 144/1770 [35:01<6:34:36, 14.56s/it]  8%|‚ñä         | 145/1770 [35:15<6:34:20, 14.56s/it]  8%|‚ñä         | 146/1770 [35:30<6:34:46, 14.59s/it]  8%|‚ñä         | 147/1770 [35:45<6:38:30, 14.73s/it]  8%|‚ñä         | 148/1770 [35:59<6:35:50, 14.64s/it]  8%|‚ñä         | 149/1770 [36:14<6:35:02, 14.62s/it]  8%|‚ñä         | 150/1770 [36:28<6:32:38, 14.54s/it]                                                      8%|‚ñä         | 150/1770 [36:28<6:32:38, 14.54s/it]  9%|‚ñä         | 151/1770 [36:43<6:32:52, 14.56s/it]  9%|‚ñä         | 152/1770 [36:57<6:31:44, 14.53s/it]  9%|‚ñä         | 153/1770 [37:12<6:35:37, 14.68s/it]  9%|‚ñä         | 154/1770 [37:27<6:34:03, 14.63s/it]  9%|‚ñâ         | 155/1770 [37:41<6:32:57, 14.60s/it]  9%|‚ñâ         | 156/1770 [37:56<6:31:02, 14.54s/it]  9%|‚ñâ         | 157/1770 [38:10<6:29:57, 14.51s/it]  9%|‚ñâ         | 158/1770 [38:25<6:29:25, 14.49s/it]  9%|‚ñâ         | 159/1770 [38:40<6:35:27, 14.73s/it]  9%|‚ñâ         | 160/1770 [38:55<6:34:19, 14.70s/it]  9%|‚ñâ         | 161/1770 [39:09<6:32:43, 14.64s/it]  9%|‚ñâ         | 162/1770 [39:24<6:31:35, 14.61s/it]  9%|‚ñâ         | 163/1770 [39:38<6:30:52, 14.59s/it]  9%|‚ñâ         | 164/1770 [39:53<6:34:18, 14.73s/it]