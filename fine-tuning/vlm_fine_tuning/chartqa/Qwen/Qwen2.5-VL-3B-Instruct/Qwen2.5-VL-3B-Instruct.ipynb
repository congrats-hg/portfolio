{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec33d57",
   "metadata": {},
   "source": [
    "- dataset: HuggingFaceM4/ChartQA\n",
    "- model: Qwen/Qwen2.5-VL-3B-Instruct  \n",
    "(used 'fine-tuning' conda environment in RTX5090 server. installed torch with `pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09003b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "# pip install torchvision pillow datasets trl transformers qwen-vl-utils peft wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1509b38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "&lt;wandb.sdk.wandb_run.Run object at 0x710c55373020&gt;"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x710c55373020>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init(mode=\"disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94145c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 메시지\n",
    "system_message = \"You are a chart analysis model that extracts precise answers from charts and graphs.\"\n",
    "\n",
    "# evaluate_prompts.py에서 가장 점수가 높은 프롬프트\n",
    "prompt = \"\"\"Based on the chart image, answer the question.\n",
    "Question: {question}\n",
    "\n",
    "Provide only the answer (number or short text), no explanation.\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc5233ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample, prompt_template):\n",
    "    \"\"\"ChartQA 데이터를 학습 포맷으로 변환\"\"\"\n",
    "    image = sample[\"image\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt_template.format(question=sample[\"query\"])},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "            },\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1q7lcscouk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 28299 samples\n",
      "Val: 1920 samples\n",
      "Test: 2500 samples\n",
      "\n",
      "Sample data:\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=422x359 at 0x710C54A6F710>, 'query': 'Is the value of Favorable 38 in 2015?', 'label': ['Yes'], 'human_or_machine': 0}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"HuggingFaceM4/ChartQA\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"val\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# 샘플 확인\n",
    "print(\"\\nSample data:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1mzo6prphxqh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 프로세서 로드\n",
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    min_pixels=512 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28,   # 동일하게 고정 (배치 안정성)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bz1g65f239s",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relaxed_match(output, expected):\n",
    "    \"\"\"ChartQA relaxed accuracy: 숫자는 ±5% 허용\"\"\"\n",
    "    output, expected = output.strip(), expected.strip()\n",
    "    if output.lower() == expected.lower():\n",
    "        return True\n",
    "    try:\n",
    "        out_num = float(output.replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "        exp_num = float(expected.replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "        if exp_num == 0:\n",
    "            return out_num == 0\n",
    "        return abs(out_num - exp_num) <= abs(exp_num) * 0.05\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def evaluate_model(model, processor, test_samples, num_samples=100):\n",
    "    \"\"\"테스트 샘플에 대해 정확도 측정\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = min(num_samples, len(test_samples))\n",
    "    \n",
    "    for i, sample in enumerate(test_samples[:total]):\n",
    "        # 추론 수행 (system + user 메시지만 사용)\n",
    "        messages = sample[\"messages\"][:2]\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # 이미지 추출\n",
    "        image = sample[\"messages\"][1][\"content\"][0][\"image\"]\n",
    "        \n",
    "        # 입력 준비\n",
    "        inputs = processor(text=[text], images=[image], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # 생성\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "        \n",
    "        # 디코딩\n",
    "        output = processor.batch_decode(\n",
    "            generated_ids[:, inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "        \n",
    "        # 정답 비교 (relaxed match)\n",
    "        expected = sample[\"messages\"][2][\"content\"][0][\"text\"].strip()\n",
    "        if relaxed_match(output, expected):\n",
    "            correct += 1\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Evaluated {i + 1}/{total} samples, current accuracy: {correct/(i+1):.2%}\")\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "i2ozpp91qih",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train formatted: 28299 samples\n",
      "Validation formatted: 1920 samples\n",
      "Test formatted: 2500 samples\n"
     ]
    }
   ],
   "source": [
    "# 최적 프롬프트로 학습/테스트 데이터 준비\n",
    "train_formatted = [format_data(row, prompt) for row in train_dataset]\n",
    "val_formatted = [format_data(row, prompt) for row in val_dataset]\n",
    "test_formatted = [format_data(row, prompt) for row in test_dataset]\n",
    "\n",
    "print(f\"Train formatted: {len(train_formatted)} samples\")\n",
    "print(f\"Validation formatted: {len(val_formatted)} samples\")\n",
    "print(f\"Test formatted: {len(test_formatted)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47oygasa44i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn 정의\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    텍스트와 이미지가 포함된 대화 데이터를 모델 학습에 적합한 형태로 변환\n",
    "    \"\"\"\n",
    "    # 1. 텍스트 전처리 - 채팅 템플릿 적용\n",
    "    texts = [processor.apply_chat_template(ex[\"messages\"], tokenize=False) for ex in examples]\n",
    "    \n",
    "    # 2. 이미지 데이터 추출 및 전처리\n",
    "    image_inputs = [process_vision_info(ex[\"messages\"])[0] for ex in examples]\n",
    "\n",
    "    # 3. 텍스트 토크나이징 + 이미지 인코딩\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # 4. 라벨 생성 (손실 계산용)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # 5. 패딩 토큰 손실 계산에서 제외\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # 6. 이미지 토큰 손실 계산에서 제외\n",
    "    image_tokens = [151652, 151653, 151655]\n",
    "    for token_id in image_tokens:\n",
    "        labels[labels == token_id] = -100\n",
    "\n",
    "    # 7. assistant 응답 이전 토큰들 마스킹 (NEW)\n",
    "    # <|im_start|>assistant\\n 토큰 시퀀스 찾기\n",
    "    assistant_start_tokens = processor.tokenizer.encode(\n",
    "        \"<|im_start|>assistant\\n\", add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    for i, input_ids in enumerate(batch[\"input_ids\"]):\n",
    "        input_ids_list = input_ids.tolist()\n",
    "        \n",
    "        # assistant 시작 위치 찾기\n",
    "        for j in range(len(input_ids_list) - len(assistant_start_tokens) + 1):\n",
    "            if input_ids_list[j:j + len(assistant_start_tokens)] == assistant_start_tokens:\n",
    "                # assistant 시작 토큰 이후부터 응답 시작\n",
    "                response_start = j + len(assistant_start_tokens)\n",
    "                # 그 이전은 모두 마스킹\n",
    "                labels[i, :response_start] = -100\n",
    "                break\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4324q9v4c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Fine-tuning 설정\n",
    "full_ft_args = SFTConfig(\n",
    "    output_dir=\"qwen25vl-chartqa-full-ft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dk0rqrkh8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py\", line 103, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/utils/generic.py\", line 918, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1476, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1257, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1170, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 430, in forward\n    hidden_states = hidden_states.reshape(seq_len, -1)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[8916, -1]' is invalid for input of size 9559040\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      2\u001b[39m full_ft_trainer = SFTTrainer(\n\u001b[32m      3\u001b[39m     model=model,\n\u001b[32m      4\u001b[39m     args=full_ft_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     processing_class=processor.tokenizer,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[43mfull_ft_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# 모델 저장\u001b[39;00m\n\u001b[32m     14\u001b[39m full_ft_trainer.save_model()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1263\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m   1262\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m-> \u001b[39m\u001b[32m1263\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:1160\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   1157\u001b[39m     inputs[\u001b[33m\"\u001b[39m\u001b[33mreturn_token_accuracy\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1158\u001b[39m     inputs[\u001b[33m\"\u001b[39m\u001b[33muse_token_scaling\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m.args.loss_type == \u001b[33m\"\u001b[39m\u001b[33mdft\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1160\u001b[39m (loss, outputs) = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[38;5;66;03m# Compute entropy\u001b[39;00m\n\u001b[32m   1165\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.use_liger_kernel:  \u001b[38;5;66;03m# liger doesn't return logits\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:197\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m    196\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/parallel/data_parallel.py:214\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[32m    213\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[32m--> \u001b[39m\u001b[32m214\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py:133\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n\u001b[32m    131\u001b[39m     output = results[i]\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m         \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m     outputs.append(output)\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/_utils.py:775\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    771\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    772\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    773\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    774\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m775\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/parallel/parallel_apply.py\", line 103, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/utils/generic.py\", line 918, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1476, in forward\n    outputs = self.model(\n              ^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1257, in forward\n    image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 1170, in get_image_features\n    image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1776, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1787, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/ryu5090/dev/miniconda3/envs/fine-tuning/lib/python3.12/site-packages/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py\", line 430, in forward\n    hidden_states = hidden_states.reshape(seq_len, -1)\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: shape '[8916, -1]' is invalid for input of size 9559040\n"
     ]
    }
   ],
   "source": [
    "# Full Fine-tuning Trainer 생성 및 학습\n",
    "full_ft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=full_ft_args,\n",
    "    train_dataset=train_formatted,\n",
    "    eval_dataset=val_formatted,\n",
    "    data_collator=collate_fn,\n",
    "    processing_class=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "full_ft_trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "full_ft_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87898bfd",
   "metadata": {},
   "source": [
    "LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020384b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA를 위해 모델 재로드 (fresh)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k251sfaik18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Fine-tuning 설정\n",
    "lora_args = SFTConfig(\n",
    "    output_dir=\"qwen25vl-chartqa-lora\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    learning_rate=1e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tx0me9m4aq7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Fine-tuning Trainer 생성 및 학습\n",
    "lora_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=lora_args,\n",
    "    train_dataset=train_formatted,\n",
    "    eval_dataset=val_formatted,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "lora_trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "lora_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85939d0e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dq6q1fwue8o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base 모델 평가\n",
    "print(\"Loading Base model...\")\n",
    "base_model_eval = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "base_accuracy = evaluate_model(base_model_eval, processor, test_formatted, num_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Fine-tuned 모델 평가\n",
    "print(\"Loading Full FT model...\")\n",
    "full_ft_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"qwen25vl-chartqa-full-ft\", \n",
    "    device_map=\"auto\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Evaluating Full FT model...\")\n",
    "full_ft_accuracy = evaluate_model(full_ft_model, processor, test_formatted, num_samples=NUM_SAMPLES)\n",
    "print(f\"\\nFull FT model accuracy: {full_ft_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltvrlfmg19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 모델 평가\n",
    "print(\"Loading LoRA model...\")\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"qwen25vl-chartqa-lora\")\n",
    "\n",
    "print(\"Evaluating LoRA model...\")\n",
    "lora_accuracy = evaluate_model(lora_model, processor, test_formatted, num_samples=NUM_SAMPLES)\n",
    "print(f\"\\nLoRA model accuracy: {lora_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tu46yigoc3l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 마크다운 표로 출력\n",
    "print(\"\\n## Results Summary\\n\")\n",
    "print(\"| Model | Accuracy |\")\n",
    "print(\"|-------|----------|\")\n",
    "print(f\"| Base | {base_accuracy:.2%} |\")\n",
    "print(f\"| Full FT | {full_ft_accuracy:.2%} |\")\n",
    "print(f\"| LoRA | {lora_accuracy:.2%} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bb233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
