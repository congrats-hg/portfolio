{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eec33d57",
   "metadata": {},
   "source": [
    "- dataset: HuggingFaceM4/ChartQA\n",
    "- model: Qwen/Qwen2.5-VL-3B-Instruct  \n",
    "(used 'fine-tuning' conda environment in RTX5090 server. installed torch with `pip install --pre torch --index-url https://download.pytorch.org/whl/nightly/cu128`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09003b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from peft import LoraConfig, PeftModel\n",
    "\n",
    "import transformers\n",
    "transformers.logging.set_verbosity_info()\n",
    "\n",
    "# pip install torchvision pillow datasets trl transformers qwen-vl-utils peft wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1509b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice:\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Create a W&B account'\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Create an account here: https://wandb.ai/authorize?signup=true&ref=models\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ryu5090/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhagyeong929\u001b[0m (\u001b[33mhagyeong929-kyonggi-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.24.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ryu5090/dev/portfolio/fine-tuning/vlm_fine_tuning/chartqa/Qwen/Qwen2.5-VL-3B-Instruct/wandb/run-20260125_152449-uugon5og</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning/runs/uugon5og' target=\"_blank\">sweet-star-1</a></strong> to <a href='https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning' target=\"_blank\">https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning/runs/uugon5og' target=\"_blank\">https://wandb.ai/hagyeong929-kyonggi-university/chartqa-finetuning/runs/uugon5og</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "&lt;wandb.sdk.wandb_run.Run object at 0x73d0d36c8e00&gt;"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x73d0d36c8e00>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# wandb.init(mode=\"disabled\")\n",
    "wandb.init(project=\"chartqa-finetuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94145c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시스템 메시지\n",
    "system_message = \"You are a chart analysis model that extracts precise answers from charts and graphs.\"\n",
    "\n",
    "# evaluate_prompts.py에서 가장 점수가 높은 프롬프트\n",
    "prompt = \"\"\"Based on the chart image, answer the question.\n",
    "Question: {question}\n",
    "\n",
    "Provide only the answer (number or short text), no explanation.\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc5233ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(sample, prompt_template):\n",
    "    \"\"\"ChartQA 데이터를 학습 포맷으로 변환\"\"\"\n",
    "    image = sample[\"image\"]\n",
    "    \n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\", \"image\": image},\n",
    "                    {\"type\": \"text\", \"text\": prompt_template.format(question=sample[\"query\"])},\n",
    "                ],\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "            },\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1q7lcscouk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 28299 samples\n",
      "Val: 1920 samples\n",
      "Test: 2500 samples\n",
      "\n",
      "Sample data:\n",
      "{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=422x359 at 0x73D0D31B7BC0>, 'query': 'Is the value of Favorable 38 in 2015?', 'label': ['Yes'], 'human_or_machine': 0}\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 로드\n",
    "dataset = load_dataset(\"HuggingFaceM4/ChartQA\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"val\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(f\"Train: {len(train_dataset)} samples\")\n",
    "print(f\"Val: {len(val_dataset)} samples\")\n",
    "print(f\"Test: {len(test_dataset)} samples\")\n",
    "\n",
    "# 샘플 확인\n",
    "print(\"\\nSample data:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1mzo6prphxqh",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/config.json\n",
      "Model config Qwen2_5_VLConfig {\n",
      "  \"architectures\": [\n",
      "    \"Qwen2_5_VLForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2048,\n",
      "  \"image_token_id\": 151655,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 128000,\n",
      "  \"max_window_layers\": 70,\n",
      "  \"model_type\": \"qwen2_5_vl\",\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 2,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"mrope_section\": [\n",
      "      16,\n",
      "      24,\n",
      "      24\n",
      "    ],\n",
      "    \"rope_type\": \"default\",\n",
      "    \"type\": \"default\"\n",
      "  },\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": 32768,\n",
      "  \"text_config\": {\n",
      "    \"architectures\": [\n",
      "      \"Qwen2_5_VLForConditionalGeneration\"\n",
      "    ],\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 151643,\n",
      "    \"dtype\": \"bfloat16\",\n",
      "    \"eos_token_id\": 151645,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 11008,\n",
      "    \"layer_types\": [\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\",\n",
      "      \"full_attention\"\n",
      "    ],\n",
      "    \"max_position_embeddings\": 128000,\n",
      "    \"max_window_layers\": 70,\n",
      "    \"model_type\": \"qwen2_5_vl_text\",\n",
      "    \"num_attention_heads\": 16,\n",
      "    \"num_hidden_layers\": 36,\n",
      "    \"num_key_value_heads\": 2,\n",
      "    \"rms_norm_eps\": 1e-06,\n",
      "    \"rope_scaling\": {\n",
      "      \"mrope_section\": [\n",
      "        16,\n",
      "        24,\n",
      "        24\n",
      "      ],\n",
      "      \"rope_type\": \"default\",\n",
      "      \"type\": \"default\"\n",
      "    },\n",
      "    \"rope_theta\": 1000000.0,\n",
      "    \"sliding_window\": null,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"use_cache\": true,\n",
      "    \"use_sliding_window\": false,\n",
      "    \"vision_token_id\": 151654,\n",
      "    \"vocab_size\": 151936\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.6\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"video_token_id\": 151656,\n",
      "  \"vision_config\": {\n",
      "    \"depth\": 32,\n",
      "    \"fullatt_block_indexes\": [\n",
      "      7,\n",
      "      15,\n",
      "      23,\n",
      "      31\n",
      "    ],\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 1280,\n",
      "    \"in_channels\": 3,\n",
      "    \"in_chans\": 3,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 3420,\n",
      "    \"model_type\": \"qwen2_5_vl\",\n",
      "    \"num_heads\": 16,\n",
      "    \"out_hidden_size\": 2048,\n",
      "    \"patch_size\": 14,\n",
      "    \"spatial_merge_size\": 2,\n",
      "    \"spatial_patch_size\": 14,\n",
      "    \"temporal_patch_size\": 2,\n",
      "    \"tokens_per_second\": 2,\n",
      "    \"window_size\": 112\n",
      "  },\n",
      "  \"vision_end_token_id\": 151653,\n",
      "  \"vision_start_token_id\": 151652,\n",
      "  \"vision_token_id\": 151654,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/model.safetensors.index.json\n",
      "Instantiating Qwen2_5_VLForConditionalGeneration model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Instantiating Qwen2_5_VisionTransformerPretrainedModel model under default dtype torch.bfloat16.\n",
      "Instantiating Qwen2_5_VLTextModel model under default dtype torch.bfloat16.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "962f8e8af1c241a3bd0243396df30673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file generation_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"repetition_penalty\": 1.05,\n",
      "  \"temperature\": 1e-06\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside Qwen/Qwen2.5-VL-3B-Instruct.\n",
      "loading configuration file preprocessor_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json\n",
      "loading configuration file preprocessor_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json\n",
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "Image processor Qwen2VLImageProcessorFast {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"disable_grouping\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessorFast\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_pixels\": 401408,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 401408,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_tensors\": null,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/vocab.json\n",
      "loading file merges.txt from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/merges.txt\n",
      "loading file tokenizer.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file video_preprocessor_config.json from cache at /home/ryu5090/.cache/huggingface/hub/models--Qwen--Qwen2.5-VL-3B-Instruct/snapshots/66285546d2b821cf421d4f5eb2576359d3770cd3/preprocessor_config.json\n",
      "Video processor Qwen2VLVideoProcessor {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"do_sample_frames\": false,\n",
      "  \"fps\": null,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_frames\": 768,\n",
      "  \"max_pixels\": 401408,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_frames\": 4,\n",
      "  \"min_pixels\": 401408,\n",
      "  \"num_frames\": null,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_metadata\": false,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2,\n",
      "  \"video_metadata\": null,\n",
      "  \"video_processor_type\": \"Qwen2VLVideoProcessor\"\n",
      "}\n",
      "\n",
      "loading configuration file processor_config.json from cache at None\n",
      "Processor Qwen2_5_VLProcessor:\n",
      "- image_processor: Qwen2VLImageProcessorFast {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"disable_grouping\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_pad\": null,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_processor_type\": \"Qwen2VLImageProcessorFast\",\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_pixels\": 401408,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_pixels\": 401408,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_tensors\": null,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2\n",
      "}\n",
      "\n",
      "- tokenizer: Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-VL-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n",
      ")\n",
      "- video_processor: Qwen2VLVideoProcessor {\n",
      "  \"crop_size\": null,\n",
      "  \"data_format\": \"channels_first\",\n",
      "  \"default_to_square\": true,\n",
      "  \"device\": null,\n",
      "  \"do_center_crop\": null,\n",
      "  \"do_convert_rgb\": true,\n",
      "  \"do_normalize\": true,\n",
      "  \"do_rescale\": true,\n",
      "  \"do_resize\": true,\n",
      "  \"do_sample_frames\": false,\n",
      "  \"fps\": null,\n",
      "  \"image_mean\": [\n",
      "    0.48145466,\n",
      "    0.4578275,\n",
      "    0.40821073\n",
      "  ],\n",
      "  \"image_std\": [\n",
      "    0.26862954,\n",
      "    0.26130258,\n",
      "    0.27577711\n",
      "  ],\n",
      "  \"input_data_format\": null,\n",
      "  \"max_frames\": 768,\n",
      "  \"max_pixels\": 401408,\n",
      "  \"merge_size\": 2,\n",
      "  \"min_frames\": 4,\n",
      "  \"min_pixels\": 401408,\n",
      "  \"num_frames\": null,\n",
      "  \"pad_size\": null,\n",
      "  \"patch_size\": 14,\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\",\n",
      "  \"resample\": 3,\n",
      "  \"rescale_factor\": 0.00392156862745098,\n",
      "  \"return_metadata\": false,\n",
      "  \"size\": {\n",
      "    \"longest_edge\": 12845056,\n",
      "    \"shortest_edge\": 3136\n",
      "  },\n",
      "  \"temporal_patch_size\": 2,\n",
      "  \"video_metadata\": null,\n",
      "  \"video_processor_type\": \"Qwen2VLVideoProcessor\"\n",
      "}\n",
      "\n",
      "\n",
      "{\n",
      "  \"processor_class\": \"Qwen2_5_VLProcessor\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 모델 및 프로세서 로드\n",
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(\n",
    "    model_id,\n",
    "    min_pixels=512 * 28 * 28,\n",
    "    max_pixels=512 * 28 * 28,   # 동일하게 고정 (배치 안정성)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bz1g65f239s",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relaxed_match(output, expected):\n",
    "    \"\"\"ChartQA relaxed accuracy: 숫자는 ±5% 허용\"\"\"\n",
    "    output, expected = output.strip(), expected.strip()\n",
    "    if output.lower() == expected.lower():\n",
    "        return True\n",
    "    try:\n",
    "        out_num = float(output.replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "        exp_num = float(expected.replace(\",\", \"\").replace(\"%\", \"\"))\n",
    "        if exp_num == 0:\n",
    "            return out_num == 0\n",
    "        return abs(out_num - exp_num) <= abs(exp_num) * 0.05\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "\n",
    "def evaluate_model(model, processor, test_samples, num_samples=100):\n",
    "    \"\"\"테스트 샘플에 대해 정확도 측정\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = min(num_samples, len(test_samples))\n",
    "    \n",
    "    for i, sample in enumerate(test_samples[:total]):\n",
    "        # 추론 수행 (system + user 메시지만 사용)\n",
    "        messages = sample[\"messages\"][:2]\n",
    "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        # 이미지 추출\n",
    "        image = sample[\"messages\"][1][\"content\"][0][\"image\"]\n",
    "        \n",
    "        # 입력 준비\n",
    "        inputs = processor(text=[text], images=[image], return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        # 생성\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
    "        \n",
    "        # 디코딩\n",
    "        output = processor.batch_decode(\n",
    "            generated_ids[:, inputs.input_ids.shape[1]:], \n",
    "            skip_special_tokens=True\n",
    "        )[0].strip()\n",
    "        \n",
    "        # 정답 비교 (relaxed match)\n",
    "        expected = sample[\"messages\"][2][\"content\"][0][\"text\"].strip()\n",
    "        if relaxed_match(output, expected):\n",
    "            correct += 1\n",
    "        \n",
    "        if (i + 1) % 20 == 0:\n",
    "            print(f\"Evaluated {i + 1}/{total} samples, current accuracy: {correct/(i+1):.2%}\")\n",
    "    \n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "i2ozpp91qih",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train formatted: 28299 samples\n",
      "Validation formatted: 1920 samples\n",
      "Test formatted: 2500 samples\n"
     ]
    }
   ],
   "source": [
    "# 최적 프롬프트로 학습/테스트 데이터 준비\n",
    "train_formatted = [format_data(row, prompt) for row in train_dataset]\n",
    "val_formatted = [format_data(row, prompt) for row in val_dataset]\n",
    "test_formatted = [format_data(row, prompt) for row in test_dataset]\n",
    "\n",
    "print(f\"Train formatted: {len(train_formatted)} samples\")\n",
    "print(f\"Validation formatted: {len(val_formatted)} samples\")\n",
    "print(f\"Test formatted: {len(test_formatted)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47oygasa44i",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn 정의\n",
    "def collate_fn(examples):\n",
    "    \"\"\"\n",
    "    텍스트와 이미지가 포함된 대화 데이터를 모델 학습에 적합한 형태로 변환\n",
    "    \"\"\"\n",
    "    # 1. 텍스트 전처리 - 채팅 템플릿 적용\n",
    "    texts = [processor.apply_chat_template(ex[\"messages\"], tokenize=False) for ex in examples]\n",
    "    \n",
    "    # 2. 이미지 데이터 추출 및 전처리\n",
    "    image_inputs = [process_vision_info(ex[\"messages\"])[0] for ex in examples]\n",
    "\n",
    "    # 3. 텍스트 토크나이징 + 이미지 인코딩\n",
    "    batch = processor(text=texts, images=image_inputs, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # 4. 라벨 생성 (손실 계산용)\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    \n",
    "    # 5. 패딩 토큰 손실 계산에서 제외\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "    # 6. 이미지 토큰 손실 계산에서 제외\n",
    "    image_tokens = [151652, 151653, 151655]\n",
    "    for token_id in image_tokens:\n",
    "        labels[labels == token_id] = -100\n",
    "\n",
    "    # 7. assistant 응답 이전 토큰들 마스킹 (NEW)\n",
    "    # <|im_start|>assistant\\n 토큰 시퀀스 찾기\n",
    "    assistant_start_tokens = processor.tokenizer.encode(\n",
    "        \"<|im_start|>assistant\\n\", add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    for i, input_ids in enumerate(batch[\"input_ids\"]):\n",
    "        input_ids_list = input_ids.tolist()\n",
    "        \n",
    "        # assistant 시작 위치 찾기\n",
    "        for j in range(len(input_ids_list) - len(assistant_start_tokens) + 1):\n",
    "            if input_ids_list[j:j + len(assistant_start_tokens)] == assistant_start_tokens:\n",
    "                # assistant 시작 토큰 이후부터 응답 시작\n",
    "                response_start = j + len(assistant_start_tokens)\n",
    "                # 그 이전은 모두 마스킹\n",
    "                labels[i, :response_start] = -100\n",
    "                break\n",
    "\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4324q9v4c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "# Full Fine-tuning 설정\n",
    "full_ft_args = SFTConfig(\n",
    "    output_dir=\"qwen25vl-chartqa-full-ft\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    learning_rate=2e-5,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    # report_to=None,\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dk0rqrkh8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You have loaded a model on multiple GPUs. `is_model_parallel` attribute will be force-set to `True` to avoid any unexpected behavior such as device placement mismatching.\n",
      "Using auto half precision backend\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n",
      "***** Running training *****\n",
      "  Num examples = 28,299\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1,769\n",
      "  Number of trainable parameters = 3,754,622,976\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='577' max='1769' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 577/1769 1:10:15 < 2:25:39, 0.14 it/s, Epoch 0.33/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.234600</td>\n",
       "      <td>0.324176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.220600</td>\n",
       "      <td>0.303575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1920\n",
      "  Batch size = 4\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1920\n",
      "  Batch size = 4\n"
     ]
    }
   ],
   "source": [
    "# Full Fine-tuning Trainer 생성 및 학습\n",
    "full_ft_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=full_ft_args,\n",
    "    train_dataset=train_formatted,\n",
    "    eval_dataset=val_formatted,\n",
    "    data_collator=collate_fn,\n",
    "    processing_class=processor.tokenizer,\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "full_ft_trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "full_ft_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87898bfd",
   "metadata": {},
   "source": [
    "LoRA fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020384b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA를 위해 모델 재로드 (fresh)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# LoRA 설정\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.05,\n",
    "    r=32,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"up_proj\", \"down_proj\", \"gate_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k251sfaik18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Fine-tuning 설정\n",
    "lora_args = SFTConfig(\n",
    "    output_dir=\"qwen25vl-chartqa-lora\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"epoch\",\n",
    "    bf16=True,\n",
    "    learning_rate=1e-4,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    remove_unused_columns=False,\n",
    "    dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    report_to=\"wandb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tx0me9m4aq7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Fine-tuning Trainer 생성 및 학습\n",
    "lora_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=lora_args,\n",
    "    train_dataset=train_formatted,\n",
    "    eval_dataset=val_formatted,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "lora_trainer.train()\n",
    "\n",
    "# 모델 저장\n",
    "lora_trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85939d0e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f50ddb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_SAMPLES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dq6q1fwue8o",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base 모델 평가\n",
    "print(\"Loading Base model...\")\n",
    "base_model_eval = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "base_accuracy = evaluate_model(base_model_eval, processor, test_formatted, num_samples=NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a3caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Fine-tuned 모델 평가\n",
    "print(\"Loading Full FT model...\")\n",
    "full_ft_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    \"qwen25vl-chartqa-full-ft\", \n",
    "    device_map=\"auto\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"Evaluating Full FT model...\")\n",
    "full_ft_accuracy = evaluate_model(full_ft_model, processor, test_formatted, num_samples=NUM_SAMPLES)\n",
    "print(f\"\\nFull FT model accuracy: {full_ft_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ltvrlfmg19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 모델 평가\n",
    "print(\"Loading LoRA model...\")\n",
    "base_model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_id, \n",
    "    device_map=\"auto\", \n",
    "    dtype=torch.bfloat16\n",
    ")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"qwen25vl-chartqa-lora\")\n",
    "\n",
    "print(\"Evaluating LoRA model...\")\n",
    "lora_accuracy = evaluate_model(lora_model, processor, test_formatted, num_samples=NUM_SAMPLES)\n",
    "print(f\"\\nLoRA model accuracy: {lora_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tu46yigoc3l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 마크다운 표로 출력\n",
    "print(\"\\n## Results Summary\\n\")\n",
    "print(\"| Model | Accuracy |\")\n",
    "print(\"|-------|----------|\")\n",
    "print(f\"| Base | {base_accuracy:.2%} |\")\n",
    "print(f\"| Full FT | {full_ft_accuracy:.2%} |\")\n",
    "print(f\"| LoRA | {lora_accuracy:.2%} |\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0bb233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tuning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
