# Document-Specialized Vision-Language Model

> LoRA fine-tuning Qwen2-VL-2B for enterprise document understanding

<!-- Badges -->
![Python](https://img.shields.io/badge/Python-3.10-3776AB?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.1-EE4C2C?logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-FFD21E?logo=huggingface&logoColor=black)
![License](https://img.shields.io/badge/License-Apache_2.0-green)

---

## Demo

<!-- TODO: Add result image/GIF after experiments -->
```
[Invoice Image]
Q: "What is the total amount?"
A: "$1,234.56" âœ“
```

---

## Highlights

| | |
|:--|:--|
| **+ğŸ’›ğŸ’›ğŸ’›%** | DocVQA improvement (vs zero-shot baseline) |
| **ğŸ’›ğŸ’›ğŸ’›M** | Trainable parameters (ğŸ’›ğŸ’›ğŸ’›% of total) |
| **ğŸ’›ğŸ’›ğŸ’›ms** | Inference latency (single image) |

---

## Motivation

Large VLMs (GPT-4V, Claude) excel at document understanding, but **cost and latency** make production deployment challenging.

This project answers three questions:

1. **Can a 2B model achieve practical document understanding?**
2. **Which document types benefit most from fine-tuning?**
3. **What are the real-world failure modes, and how can we address them?**

---

## Quick Start

```bash
pip install torch transformers peft qwen-vl-utils
```

```python
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import PeftModel

# Load
model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
model = PeftModel.from_pretrained(model, "ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora")
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# See scripts/inference.py for full inference code
```

---

## Results

### Main Benchmarks

| Benchmark | Baseline | Ours | Î” |
|-----------|----------|------|---|
| DocVQA (ANLS) | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | **+ğŸ’›ğŸ’›ğŸ’›** |
| ChartQA (Acc) | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | **+ğŸ’›ğŸ’›ğŸ’›** |
| InfoVQA (ANLS) | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | **+ğŸ’›ğŸ’›ğŸ’›** |

> Baseline = Qwen2-VL-2B-Instruct zero-shot

### Comparison with Other Models

| Model | Size | DocVQA |
|-------|------|--------|
| GPT-4V | - | ğŸ’›ğŸ’›ğŸ’› |
| Qwen2-VL-7B | 7B | ğŸ’›ğŸ’›ğŸ’› |
| **Ours** | **2B** | **ğŸ’›ğŸ’›ğŸ’›** |

---

## Tech Stack

![Qwen2-VL](https://img.shields.io/badge/Base-Qwen2--VL--2B-9B59B6)
![LoRA](https://img.shields.io/badge/Method-LoRA-E67E22)
![PEFT](https://img.shields.io/badge/Library-PEFT-3498DB)
![Wandb](https://img.shields.io/badge/Tracking-Wandb-FFCC00)

---

## Project Structure

```
â”œâ”€â”€ configs/          # Training configurations
â”œâ”€â”€ scripts/          # Train/eval/inference scripts
â”œâ”€â”€ src/              # Dataset, model, utilities
â”œâ”€â”€ notebooks/        # Analysis notebooks
â””â”€â”€ results/          # Experiment results
```

---

## Key Takeaways

<!-- TODO: Fill after experiments -->
- ğŸ’›ğŸ’›ğŸ’› (e.g., "Understanding the trade-off between LoRA rank and performance")
- ğŸ’›ğŸ’›ğŸ’› (e.g., "Impact of document type imbalance on model performance")
- ğŸ’›ğŸ’›ğŸ’› (e.g., "Characteristics and limitations of ANLS metric for VLM evaluation")

---

## Limitations & Future Work

**Limitations**
- English documents only (multilingual not evaluated)
- Single-page documents only
- Single seed (42) results

**Future Work**
- [ ] Multilingual support (Korean, Chinese)
- [ ] Multi-page document understanding
- [ ] Quantization for edge deployment

---

<details>
<summary><strong>Technical Details</strong> (Click to expand)</summary>

### Model Configuration

| | |
|---|---|
| Base Model | Qwen2-VL-2B-Instruct |
| Fine-tuning | LoRA (r=ğŸ’›ğŸ’›ğŸ’›, Î±=ğŸ’›ğŸ’›ğŸ’›) |
| Target Modules | q_proj, k_proj, v_proj, o_proj |
| Vision Encoder | Frozen |
| Trainable Params | ğŸ’›ğŸ’›ğŸ’›M (ğŸ’›ğŸ’›ğŸ’›%) |

### Training Setup

```yaml
training:
  epochs: ğŸ’›ğŸ’›ğŸ’›
  batch_size: ğŸ’›ğŸ’›ğŸ’›
  learning_rate: ğŸ’›ğŸ’›ğŸ’›
  scheduler: cosine

data:
  min_pixels: ğŸ’›ğŸ’›ğŸ’›
  max_pixels: ğŸ’›ğŸ’›ğŸ’›
```

### Dataset

| Source | Samples | Type |
|--------|---------|------|
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› |
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› |
| **Total** | **ğŸ’›ğŸ’›ğŸ’›** | - |

### Evaluation Protocol

- **Metrics**: ANLS (DocVQA, InfoVQA), Relaxed Accuracy (ChartQA)
- **Baseline**: Qwen2-VL-2B-Instruct zero-shot
- **Data Leakage**: Verified no overlap between train/eval splits

### Performance by Document Type

| Type | Baseline | Ours | Î” |
|------|----------|------|---|
| Forms | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | +ğŸ’›ğŸ’›ğŸ’› |
| Tables | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | +ğŸ’›ğŸ’›ğŸ’› |
| Invoices | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | +ğŸ’›ğŸ’›ğŸ’› |

### Ablation: LoRA Rank

| Rank | DocVQA | Params |
|------|--------|--------|
| 16 | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›M |
| 32 | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›M |
| 64 | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›M |

### Failure Analysis

| Error Type | Frequency | Example |
|------------|-----------|---------|
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›% | ğŸ’›ğŸ’›ğŸ’› |
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›% | ğŸ’›ğŸ’›ğŸ’› |

### Inference Performance

| GPU | Latency | Cost/1K images |
|-----|---------|----------------|
| A100 | ğŸ’›ğŸ’›ğŸ’› ms | $ğŸ’›ğŸ’›ğŸ’› |
| T4 | ğŸ’›ğŸ’›ğŸ’› ms | $ğŸ’›ğŸ’›ğŸ’› |
| GPT-4V API | ğŸ’›ğŸ’›ğŸ’› ms | $ğŸ’›ğŸ’›ğŸ’› |

</details>

---

<details>
<summary><strong>Reproducibility</strong> (Click to expand)</summary>

### Environment

```
Python: 3.10.12
CUDA: 12.1
OS: Ubuntu 22.04 LTS
```

### Requirements

```
torch==2.1.2
transformers==4.37.2
peft==0.7.1
accelerate==0.25.0
datasets==2.16.1
wandb==0.16.2
qwen-vl-utils==0.0.2
```

### Training

```bash
python scripts/train.py --config configs/document_lora.yaml
```

### Evaluation

```bash
python scripts/evaluate.py \
    --model_path outputs/checkpoint-final \
    --benchmarks docvqa chartqa infovqa
```

### Full Inference Code

```python
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import PeftModel
from PIL import Image
from qwen_vl_utils import process_vision_info

# Load model
base_model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, "ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora")
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# Prepare input
image = Image.open("invoice.png")
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "What is the invoice total?"},
        ],
    }
]

# Process
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt"
).to(model.device)

# Generate
output = model.generate(**inputs, max_new_tokens=256)
generated_ids = output[:, inputs.input_ids.shape[1]:]
print(processor.batch_decode(generated_ids, skip_special_tokens=True)[0])
```

</details>

---

## License

Apache 2.0

---

## Links

**Model**: [huggingface.co/ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora](https://huggingface.co/ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora)
**Wandb**: [wandb.ai/ğŸ’›ğŸ’›ğŸ’›/document-vlm](https://wandb.ai/ğŸ’›ğŸ’›ğŸ’›/document-vlm)
**Contact**: ğŸ’›ğŸ’›ğŸ’›
