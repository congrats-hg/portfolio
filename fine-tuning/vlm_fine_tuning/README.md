# Document-Specialized Vision-Language Model

> Qwen2-VL-2Bë¥¼ ë¬¸ì„œ ì´í•´ì— íŠ¹í™”ì‹œí‚¨ LoRA fine-tuning í”„ë¡œì íŠ¸

<!-- Badges -->
![Python](https://img.shields.io/badge/Python-3.10-3776AB?logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.1-EE4C2C?logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/HuggingFace-Transformers-FFD21E?logo=huggingface&logoColor=black)
![License](https://img.shields.io/badge/License-Apache_2.0-green)

---

## Demo

<!-- TODO: ì‹¤í—˜ ì™„ë£Œ í›„ ê²°ê³¼ ì´ë¯¸ì§€/GIF ì¶”ê°€ -->
```
[Invoice ì´ë¯¸ì§€]
Q: "What is the total amount?"
A: "$1,234.56" âœ“
```

---

## Highlights

| | |
|:--|:--|
| **+ğŸ’›ğŸ’›ğŸ’›%** | DocVQA ì„±ëŠ¥ í–¥ìƒ (vs zero-shot baseline) |
| **ğŸ’›ğŸ’›ğŸ’›M** | í•™ìŠµ íŒŒë¼ë¯¸í„° (ì „ì²´ì˜ ğŸ’›ğŸ’›ğŸ’›%) |
| **ğŸ’›ğŸ’›ğŸ’›ms** | ì¶”ë¡  ì†ë„ (single image) |

---

## Why This Project?

ëŒ€í˜• VLM(GPT-4V, Claude)ì€ ë¬¸ì„œ ì´í•´ì— ê°•ë ¥í•˜ì§€ë§Œ, **ë¹„ìš©ê³¼ ì§€ì—°ì‹œê°„** ë¬¸ì œë¡œ ì‹¤ë¬´ ë„ì…ì´ ì–´ë µìŠµë‹ˆë‹¤.

ì´ í”„ë¡œì íŠ¸ëŠ” ì„¸ ê°€ì§€ ì§ˆë¬¸ì— ë‹µí•©ë‹ˆë‹¤:

1. **2B ëª¨ë¸ë¡œë„ ì‹¤ìš©ì ì¸ ë¬¸ì„œ ì´í•´ê°€ ê°€ëŠ¥í•œê°€?**
2. **ì–´ë–¤ ë¬¸ì„œ ìœ í˜•ì—ì„œ fine-tuning íš¨ê³¼ê°€ í°ê°€?**
3. **ì‹¤ì œ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ëŠ” ë¬´ì—‡ì´ê³ , ì–´ë–»ê²Œ ê°œì„ í•  ìˆ˜ ìˆëŠ”ê°€?**

---

## Quick Start

```bash
pip install torch transformers peft qwen-vl-utils
```

```python
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import PeftModel

# Load
model = Qwen2VLForConditionalGeneration.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
model = PeftModel.from_pretrained(model, "ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora")
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# ì „ì²´ ì¶”ë¡  ì½”ë“œëŠ” scripts/inference.py ì°¸ì¡°
```

---

## Results

### Main Benchmark

| Benchmark | Baseline | Ours | Î” |
|-----------|----------|------|---|
| DocVQA (ANLS) | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | **+ğŸ’›ğŸ’›ğŸ’›** |
| ChartQA (Acc) | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | **+ğŸ’›ğŸ’›ğŸ’›** |
| InfoVQA (ANLS) | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | **+ğŸ’›ğŸ’›ğŸ’›** |

> Baseline = Qwen2-VL-2B-Instruct zero-shot

### vs Other Models

| Model | Size | DocVQA |
|-------|------|--------|
| GPT-4V | - | ğŸ’›ğŸ’›ğŸ’› |
| Qwen2-VL-7B | 7B | ğŸ’›ğŸ’›ğŸ’› |
| **Ours** | **2B** | **ğŸ’›ğŸ’›ğŸ’›** |

---

## Tech Stack

![Qwen2-VL](https://img.shields.io/badge/Base-Qwen2--VL--2B-9B59B6)
![LoRA](https://img.shields.io/badge/Method-LoRA-E67E22)
![PEFT](https://img.shields.io/badge/Library-PEFT-3498DB)
![Wandb](https://img.shields.io/badge/Tracking-Wandb-FFCC00)

---

## Project Structure

```
â”œâ”€â”€ configs/          # í•™ìŠµ ì„¤ì •
â”œâ”€â”€ scripts/          # í•™ìŠµ/í‰ê°€/ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ src/              # ë°ì´í„°ì…‹, ëª¨ë¸, ìœ í‹¸ë¦¬í‹°
â”œâ”€â”€ notebooks/        # ë¶„ì„ ë…¸íŠ¸ë¶
â””â”€â”€ results/          # ì‹¤í—˜ ê²°ê³¼
```

---

## What I Learned

<!-- TODO: ì‹¤í—˜ í›„ ì‘ì„± -->
- ğŸ’›ğŸ’›ğŸ’› (e.g., "LoRA rankì™€ ì„±ëŠ¥ì˜ trade-off ê´€ê³„ ì´í•´")
- ğŸ’›ğŸ’›ğŸ’› (e.g., "ë¬¸ì„œ ìœ í˜•ë³„ ë°ì´í„° ë¶ˆê· í˜•ì´ ì„±ëŠ¥ì— ë¯¸ì¹˜ëŠ” ì˜í–¥")
- ğŸ’›ğŸ’›ğŸ’› (e.g., "VLM í‰ê°€ ë©”íŠ¸ë¦­(ANLS) íŠ¹ì„±ê³¼ í•œê³„")

---

## Limitations & Future Work

**Limitations**
- ì˜ì–´ ë¬¸ì„œë§Œ í‰ê°€ (ë‹¤êµ­ì–´ ë¯¸ì§€ì›)
- ë‹¨ì¼ í˜ì´ì§€ ë¬¸ì„œë§Œ ì²˜ë¦¬
- Single seed (42) ê²°ê³¼

**Future Work**
- [ ] ë‹¤êµ­ì–´ ë¬¸ì„œ ì§€ì› (í•œêµ­ì–´, ì¤‘êµ­ì–´)
- [ ] Multi-page ë¬¸ì„œ ì²˜ë¦¬
- [ ] ì–‘ìí™”ë¥¼ í†µí•œ Edge ë°°í¬

---

<details>
<summary><strong>Technical Details</strong> (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

### Model Configuration

| | |
|---|---|
| Base Model | Qwen2-VL-2B-Instruct |
| Fine-tuning | LoRA (r=ğŸ’›ğŸ’›ğŸ’›, Î±=ğŸ’›ğŸ’›ğŸ’›) |
| Target Modules | q_proj, k_proj, v_proj, o_proj |
| Vision Encoder | Frozen |
| Trainable Params | ğŸ’›ğŸ’›ğŸ’›M (ğŸ’›ğŸ’›ğŸ’›%) |

### Training Setup

```yaml
training:
  epochs: ğŸ’›ğŸ’›ğŸ’›
  batch_size: ğŸ’›ğŸ’›ğŸ’›
  learning_rate: ğŸ’›ğŸ’›ğŸ’›
  scheduler: cosine

data:
  min_pixels: ğŸ’›ğŸ’›ğŸ’›
  max_pixels: ğŸ’›ğŸ’›ğŸ’›
```

### Dataset

| Source | Samples | Type |
|--------|---------|------|
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› |
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› |
| **Total** | **ğŸ’›ğŸ’›ğŸ’›** | - |

### Evaluation Protocol

- **Metrics**: ANLS (DocVQA, InfoVQA), Relaxed Accuracy (ChartQA)
- **Baseline**: Qwen2-VL-2B-Instruct zero-shot
- **Data Leakage**: Train/eval ë°ì´í„° ë¶„ë¦¬ ê²€ì¦ ì™„ë£Œ

### Performance by Document Type

| Type | Baseline | Ours | Î” |
|------|----------|------|---|
| Forms | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | +ğŸ’›ğŸ’›ğŸ’› |
| Tables | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | +ğŸ’›ğŸ’›ğŸ’› |
| Invoices | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’› | +ğŸ’›ğŸ’›ğŸ’› |

### Ablation: LoRA Rank

| Rank | DocVQA | Params |
|------|--------|--------|
| 16 | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›M |
| 32 | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›M |
| 64 | ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›M |

### Failure Analysis

| Error Type | Frequency | Example |
|------------|-----------|---------|
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›% | ğŸ’›ğŸ’›ğŸ’› |
| ğŸ’›ğŸ’›ğŸ’› | ğŸ’›ğŸ’›ğŸ’›% | ğŸ’›ğŸ’›ğŸ’› |

### Inference Performance

| GPU | Latency | Cost/1K images |
|-----|---------|----------------|
| A100 | ğŸ’›ğŸ’›ğŸ’› ms | $ğŸ’›ğŸ’›ğŸ’› |
| T4 | ğŸ’›ğŸ’›ğŸ’› ms | $ğŸ’›ğŸ’›ğŸ’› |
| GPT-4V API | ğŸ’›ğŸ’›ğŸ’› ms | $ğŸ’›ğŸ’›ğŸ’› |

</details>

---

<details>
<summary><strong>Reproducibility</strong> (í´ë¦­í•˜ì—¬ í¼ì¹˜ê¸°)</summary>

### Environment

```
Python: 3.10.12
CUDA: 12.1
OS: Ubuntu 22.04 LTS
```

### Requirements

```
torch==2.1.2
transformers==4.37.2
peft==0.7.1
accelerate==0.25.0
datasets==2.16.1
wandb==0.16.2
qwen-vl-utils==0.0.2
```

### Training

```bash
python scripts/train.py --config configs/document_lora.yaml
```

### Evaluation

```bash
python scripts/evaluate.py \
    --model_path outputs/checkpoint-final \
    --benchmarks docvqa chartqa infovqa
```

### Full Inference Code

```python
import torch
from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
from peft import PeftModel
from PIL import Image
from qwen_vl_utils import process_vision_info

# Load model
base_model = Qwen2VLForConditionalGeneration.from_pretrained(
    "Qwen/Qwen2-VL-2B-Instruct",
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
model = PeftModel.from_pretrained(base_model, "ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora")
processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")

# Prepare input
image = Image.open("invoice.png")
messages = [
    {
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": "What is the invoice total?"},
        ],
    }
]

# Process
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
image_inputs, video_inputs = process_vision_info(messages)
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    padding=True,
    return_tensors="pt"
).to(model.device)

# Generate
output = model.generate(**inputs, max_new_tokens=256)
generated_ids = output[:, inputs.input_ids.shape[1]:]
print(processor.batch_decode(generated_ids, skip_special_tokens=True)[0])
```

</details>

---

## License

Apache 2.0

---

## Links

**Model**: [huggingface.co/ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora](https://huggingface.co/ğŸ’›ğŸ’›ğŸ’›/document-vlm-lora)
**Wandb**: [wandb.ai/ğŸ’›ğŸ’›ğŸ’›/document-vlm](https://wandb.ai/ğŸ’›ğŸ’›ğŸ’›/document-vlm)
**Contact**: ğŸ’›ğŸ’›ğŸ’›
